## üßÆ Explica√ß√£o do C√≥digo: Hash Table com Normaliza√ß√£o

-----

### 1\. Pr√©-processamento: `normalize_string`

Esta fun√ß√£o √© crucial para garantir que o hashing trate nomes semelhantes, mas com ou sem acentos, da mesma forma (ex: "Jo√£o" e "Joao").

  * **`import unicodedata`**: Importa o m√≥dulo para trabalhar com a Base de Dados de Caracteres Unicode.
  * **`unicodedata.normalize('NFD', s)`**: Aplica a forma de **Normaliza√ß√£o por Decomposi√ß√£o Can√¥nica (NFD)**. Isso separa caracteres acentuados em sua **letra base** e seu **acento** (o marcador diacr√≠tico).
      * *Exemplo:* '√°' se torna 'a' + 'ÃÅ' (o acento agudo).
  * **`unicodedata.category(c) != 'Mn'`**: Filtra os caracteres. `'Mn'` significa "Mark, Nonspacing" (Marca, Sem Espa√ßamento), que s√£o os acentos.
      * Ao filtrar os 'Mn', a fun√ß√£o **remove todos os acentos**.
  * **`.lower()`**: Converte a string resultante para **min√∫sculas**.
  * **Resultado:** A string √© padronizada, tornando o hash **insens√≠vel a acentos e case-insensitive** (n√£o diferencia mai√∫sculas de min√∫sculas).
      * *Exemplo:* `normalize_string("M√°rcio")` retorna `"marcio"`.

-----

### 2\. Fun√ß√£o de Hashing Personalizada: `custom_hash`

Esta fun√ß√£o calcula o valor hash usando o algoritmo **Polynomial Rolling Hash**.

  * **Normaliza√ß√£o**: Primeiro, ela normaliza o `name` usando `normalize_string(name)`.
  * **Inicializa√ß√£o**:
      * `hash_value = 0`: O valor hash acumulado.
      * `base = 131`: Uma base prima escolhida para garantir uma boa distribui√ß√£o.
      * `mod = 2**61 - 1`: Um primo grande (Mersenne-like) para o m√≥dulo, usado para evitar *overflow* e manter a distribui√ß√£o uniforme.
  * **C√°lculo do Hash (Loop)**:
    ```python
    hash_value = (hash_value * base + ord(char)) % mod
    ```
      * Para cada caractere (`char`):
        1.  O `hash_value` anterior √© multiplicado pela `base`.
        2.  O valor num√©rico do caractere (`ord(char)`, geralmente o c√≥digo ASCII/Unicode) √© adicionado.
        3.  Tudo √© calculado **m√≥dulo `mod`**.
  * **Mapeamento para o Tamanho da Tabela**:
    ```python
    return hash_value % table_size
    ```
      * O hash final, que √© um n√∫mero muito grande, √© calculado **m√≥dulo `table_size`** para produzir um √≠ndice v√°lido dentro do array da tabela hash (de `0` a `table_size - 1`).

-----

### 3\. Classe da Tabela Hash: `HashTable`

Esta classe gerencia a estrutura de dados da Tabela Hash.

  * **`__init__(self, size)`**:
      * Cria a lista interna `self.table`, que √© uma lista de listas (ou *buckets*). Cada *bucket* √© inicializado como uma lista vazia (`[[] for _ in range(size)]`).
      * `self.collisions = 0`: Contador para rastrear o n√∫mero de colis√µes.
  * **`insert(self, key)`**:
    1.  **Calcula o √çndice**: `index = custom_hash(key, self.size)` usa a chave (`key`) para encontrar a posi√ß√£o correta.
    2.  **Verifica Colis√£o**: `if self.table[index]: self.collisions += 1`
          * Se o *bucket* no `index` j√° contiver elementos, significa que ocorreu uma **colis√£o**.
    3.  **Encadeamento (Chaining)**: `self.table[index].append(key)`
          * Adiciona a nova `key` √† lista no √≠ndice calculado. Este √© o m√©todo de **encadeamento**, onde m√∫ltiplas chaves que mapeiam para o mesmo √≠ndice s√£o armazenadas na mesma lista.

-----

### 4\. Execu√ß√£o e An√°lise

O c√≥digo, em seguida, demonstra o uso:

1.  **Inicializa√ß√£o**: Cria a tabela com `tamanho_tabela = 16`.
2.  **Inser√ß√£o**: Itera sobre `nomes_teste`, inserindo-os e exibindo o √≠ndice.
      * *Note as colis√µes intencionais:* Nomes como "Jo√£o" e "Jo√£o Silva" ou "Marcos" e "Marcus" tendem a colidir devido √† similaridade de seus hashes.
3.  **An√°lise Final**:
      * Exibe a estrutura da tabela (√≠ndices com suas listas de chaves).
      * Mostra o **Total de colis√µes**.
      * Calcula o **Load Factor** ($\alpha$), que √© a raz√£o entre o n√∫mero de itens (`len(nomes_teste)`) e o tamanho da tabela (`tamanho_tabela`).
          * Um *load factor* alto ($> 1$) ou muito pr√≥ximo de $1$ indica que a tabela est√° ficando cheia e as colis√µes s√£o mais prov√°veis, o que pode degradar a performance de busca.